{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BELOW ARE ALL THE FUNCTIONS OR FEATURES IMPLEMENTED\n",
    "'''\n",
    "\n",
    "import nltk, re, language_tool_python\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import fileinput, statistics\n",
    "from nltk import Tree\n",
    "import csv\n",
    "\n",
    "'''\n",
    "Simple code below to open and read the files\n",
    "'''\n",
    "report = open(\"/home/praise_1906/Downloads/2019-calculus-RR03-0499.txt\").read()\n",
    "report_lines = open(\"/home/praise_1906/Downloads/2019-calculus-RR03-0499.txt\").readlines()\n",
    "rubric = open(\"/home/praise_1906/Downloads/pendulum_rubric.txt\").read()\n",
    "rubric_lines = open(\"/home/praise_1906/Downloads/pendulum_rubric.txt\").readlines()\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    \n",
    "'''\n",
    "This function returns the length of the report\n",
    "'''\n",
    "def length_report(input_report):\n",
    "    tokens = nltk.word_tokenize(input_report)\n",
    "    return len(tokens)\n",
    "\n",
    "'''\n",
    "This function counts the number of formulaes in the report\n",
    "'''\n",
    "def count_formulas(input_report):\n",
    "    count_fr = 0\n",
    "    for line in input_report:\n",
    "        if line.startswith('') and line.endswith(''):\n",
    "            if line.count(\"#\") == 2:\n",
    "                count_fr += 1\n",
    "    return count_fr\n",
    "\n",
    "'''\n",
    "This function counts the number of grammatical and other errors in the report\n",
    "'''\n",
    "def count_errors(input_report):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    tokens = nltk.sent_tokenize(input_report)\n",
    "    # Below is a simple test case which can be used to verify \n",
    "    # tokens = ['A sentence with a error in the Hitchhiker’s Guide tot he Galaxy', 'This is a perfectly normal sentence']\n",
    "    res = 0\n",
    "    for text in tokens:\n",
    "        matches = tool.check(text)\n",
    "        res += len(matches)\n",
    "    return res\n",
    "\n",
    "'''\n",
    "This function compares the report and the rubric and finds overlapping words between them\n",
    "and returns the total number of overlapping words\n",
    "\n",
    "To achieve an even higher accuracy: use the word2vec to convert the report to vectors and the rubric to vectors and then\n",
    "take the average of all the words in each sentence and calculate the cosine between the\n",
    "resulting embeddings\n",
    "'''\n",
    "def task_sim_ft(input_report, rubric):\n",
    "    tokens_report = nltk.word_tokenize(input_report)\n",
    "    tokens_rubric = nltk.word_tokenize(rubric)\n",
    "    overlapping_words = set(tokens_report) & set(tokens_rubric)\n",
    "    return len(overlapping_words)\n",
    "\n",
    "'''\n",
    "This function checks for grammar or spelling errors using the language tool (2.7 - Error Features)\n",
    "and returns the errors with a possible solution/replacement\n",
    "'''\n",
    "def error_ft(input_report):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    tokens = nltk.sent_tokenize(input_report)\n",
    "    # Below is a simple test case which can be used to verify\n",
    "    # tokens = ['A sentence with a error in the Hitchhiker’s Guide tot he Galaxy', 'This is a perfectly normal sentence']\n",
    "    for text in tokens:\n",
    "        matches = tool.check(text)\n",
    "        if matches != []:\n",
    "            print(matches)\n",
    "\n",
    "'''\n",
    "This function analyzes the sentiments of the statements using pycorenlp and nlp.annotate.\n",
    "Then we convert the sentences to parse trees or syntactic trees using pycorenlp.\n",
    "We focus on the syntactic features here which are explained in the function\n",
    "'''\n",
    "def syntax_ft(input_report):  \n",
    "    text, parsed_str = \"\", \"\"\n",
    "    depth, num_sentences = 0, 0\n",
    "    mean_var_lst = []\n",
    "    \n",
    "    num_commas, num_prep = 0, 0\n",
    "    num_modal = 0\n",
    "    \n",
    "    for line in input_report:\n",
    "        \n",
    "        # This piece of code puts each sentence in new line when it finds a full stop\n",
    "        pat = ('(?<!Dr)(?<!Esq)\\. +(?=[A-Z])')\n",
    "        list_string = ''.join(re.sub(pat, '.\\n', line))\n",
    "        tokens = nltk.sent_tokenize(list_string)\n",
    "        \n",
    "        for token in tokens:\n",
    "            words = nltk.word_tokenize(token)\n",
    "            temp_lst = nltk.pos_tag(words)\n",
    "            \n",
    "            # The two for loops below allow us to calculate the modal verb and prepositions\n",
    "            # and the number of commans in each sentence respectively (tells us the total sum)\n",
    "            for i in range(0, len(temp_lst)):\n",
    "                if temp_lst[i][1] == 'MD':\n",
    "                    num_modal += 1\n",
    "                if temp_lst[i][1] == 'IN':\n",
    "                    num_prep += 1\n",
    "            for word in words:\n",
    "                if word == ',':\n",
    "                    num_commas += 1\n",
    "                    \n",
    "            # This particular if statement focuses on the \"Statistics of sentence length\" feature\n",
    "            # which is one of the four syntactical features (Chen and He, 2013)\n",
    "            if len(words) > 10 or len(words) > 18 or len(words) > 25:\n",
    "                mean_var_lst.append(len(words))\n",
    "                num_sentences += 1\n",
    "                \n",
    "            # The piece of code below is used to find the parse tree from the tokens for each word in the .txt file\n",
    "            if token == \"\\n\":\n",
    "                continue\n",
    "            else:\n",
    "                text = \"\"\n",
    "                text += token\n",
    "                output = nlp.annotate(text, properties={\n",
    "                   'annotators': 'parse',\n",
    "                   'outputFormat': 'json'\n",
    "                })\n",
    "                parsed_str += output['sentences'][0]['parse']\n",
    "    \n",
    "    # These print statements are to check the output of the \"Statistics of sentence length\" feature\n",
    "    print(mean_var_lst)\n",
    "    print(statistics.variance(mean_var_lst))\n",
    "    print(statistics.mean(mean_var_lst))\n",
    "    \n",
    "    parsed_sent = nltk.sent_tokenize(parsed_str)\n",
    "\n",
    "    sbar_lst, node_depth = [], []\n",
    "    \n",
    "    # Inside this for loop, we check for \"SBAR\" which allows us to count the number of\n",
    "    # subclauses in each sentence and we store it in a list to calculate the mean\n",
    "    for sent in parsed_sent:\n",
    "        node_depth.append(sent.count('\\n'))\n",
    "        sent_words = nltk.word_tokenize(sent)  \n",
    "        for word in sent_words:\n",
    "            if word == 'SBAR':\n",
    "                sbar_lst.append(sent_words.count(word))\n",
    "                \n",
    "            # This particular piece of code is useful to locate the roots and belongs to the\n",
    "            # \"Sentence level\" feature in the syntactical feature part. It helps us calculate the\n",
    "            # depth and height of the parse tree\n",
    "            if word == 'ROOT':\n",
    "                depth += 1\n",
    "    \n",
    "    # To calculate the mean of the number of subclauses in each sentence\n",
    "    print(statistics.mean(sbar_lst))\n",
    "    \n",
    "    # To calculate the node depth and height of the parse tree\n",
    "    print(node_depth)\n",
    "    print(\"Height of the parse tree is:\", max(node_depth) + 1)\n",
    "    print(sum(node_depth))\n",
    "    \n",
    "    # To print the number of modes, prepositions and commas in each sentence (sum)\n",
    "    print(num_commas)\n",
    "    print(num_modal)\n",
    "    print(num_prep)\n",
    "    \n",
    "    # returns the depth of the parse tree\n",
    "    return depth\n",
    "\n",
    "'''\n",
    "This function calls or gets all the above defined functions and is to be used in the SVM\n",
    "'''\n",
    "def get_feature(input_report, input_report_lines, input_rubric):\n",
    "    length_report(input_report)\n",
    "    count_formulas(input_report_lines)\n",
    "    count_errors(input_report)\n",
    "    task_sim_ft(input_report, input_rubric)\n",
    "    error_ft(input_report)\n",
    "    syntax_ft(input_report_lines)\n",
    "\n",
    "'''Below you can test the functions individually'''\n",
    "\n",
    "#length_report(report)\n",
    "#count_formulas(report_lines)\n",
    "#count_errors(report)\n",
    "#task_sim_ft(report, rubric)\n",
    "#error_ft(report)\n",
    "#syntax_ft(report_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
